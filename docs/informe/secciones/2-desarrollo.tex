\section{Desarrollo}

  \subsection{Conceptos teóricos}

    La resolución de \emph{sistemas de ecuaciones lineales} es un problema recurrente en el análisis numérico, ya que estos sistemas son útiles a la hora de modelar matemáticamente el comportamiento de problemas provenientes de diversas disciplinas, como la física y la ingeniería, en forma computacional. Es por eso que resulta de interés encontrar algoritmos que permitan, de manera tan eficiente como sea posible, encontrar soluciones a estos sistemas.

    Un sistema de $m$ ecuaciones lineales con $n$ incógnitas tiene la forma
    \[ \left\lbrace \begin{matrix}
        a_{11} \, x_1 & + & a_{12} \, x_2 & + & \dots  & + & a_{1n} \, x_n & = & b_1    \\
        a_{21} \, x_1 & + & a_{22} \, x_2 & + & \dots  & + & a_{2n} \, x_n & = & b_2    \\
        \vdots        &   & \vdots        &   & \vdots &   & \vdots        &   & \vdots \\
        a_{m1} \, x_1 & + & a_{m2} \, x_2 & + & \dots  & + & a_{mn} \, x_n & = & b_m    \\
    \end{matrix} \right. \]
    donde $a_{ij}$, $b_i$ para $i = 1, \dots, m$, $j = 1, \dots, n$, son constantes reales.
    % Resolver el sistema equivale a encontrar un conjunto de valores para las incógnitas $x_1, \dots, x_n$ de forma que todas las ecuaciones se satisfagan simultáneamente. 
    Una manera frecuente de representar este sistema, que será utilizada en este trabajo, es la matricial, que consiste en expresarlo en la forma
    \[ \mat{A}x = b \]
    donde 
    \[ \mat{A} = \left[ \begin{matrix} a_{11} & a_{12} & \dots  & a_{1n} \\
                                       a_{21} & a_{22} & \dots  & a_{2n} \\
                                       \vdots & \vdots & \ddots & \vdots \\
                                       a_{n1} & a_{n2} & \dots  & a_{nn} \\ \end{matrix} \right] \in \mathbb{R}^{m \times n} \qquad
    x = \left[ \begin{matrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{matrix} \right] \in {R}^n \qquad
    b = \left[ \begin{matrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{matrix} \right] \in {R}^m \]

      Una propiedad a destacar es que, dado un sistema de ecuaciones lineales, la aplicación de ciertas operaciones sobre las ecuaciones permite obtener un sistema equivalente, es decir, que tiene las mismas soluciones. Más aún, puede probarse que todo sistema es equivalente a otro que se encuentra en forma \emph{triangular} o \emph{reducida} (ver \cite[p.~358]{burden}). Este resultado es importante porque, dado un sistema en forma triangular (superior o inferior), encontrar sus soluciones es sumamente sencillo. Para lograrlo, se utilizan un par de algoritmos simples, el de \emph{sustitución hacia adelante} y el de \emph{sustitución hacia atrás}.

      El algoritmo de \emph{sustitución hacia adelante} considera un sistema de ecuaciones $\mat{L}x=b$, donde $\mat{L}$ es una matriz triangular inferior, y devuelve un vector $s$ solución del sistema. Su complejidad es cuadrática en la cantidad de incógnitas. Lo hace aprovechando el hecho de que el valor de la primera incógnita puede ser despejado de forma inmediata; luego, sustituyendo este valor en la segunda ecuación, puede despejarse la segunda incógnita, y así sucesivamente hasta resolver el sistema completo. El pseudocódigo del algoritmo es el siguiente:

      \vspace*{1em}
      \begin{algorithm}[H]
        \caption{Sustitución hacia adelante}
        \SetKwFunction{sustAdelante}{Sustitución hacia adelante}
        \KwData{$\mat{A} \in \mathbb{R}^{n \times n}, b \in \mathbb{R}^n$}
        \KwResult{$s \in \mathbb{R}^n$, solución del sistema $Ax=b$, considerando a $\mat{A}$ como una matriz triangular inferior}
        \For{$i \gets 1$ \KwTo $n$}{
          $suma \gets 0$ \;
          \For{$j \gets 1$ \KwTo $i$}{
            $suma \gets suma + s_j \cdot \mat{A}_{i,j}$ \;
          }
          $s_i \gets \frac{b_i - suma}{\mat{A}_{i,i}}$
        }
      \end{algorithm}
      \vspace*{1em}

      El algoritmo de \emph{sustitución hacia atrás} es análogo al anterior, pero considera un sistema de ecuaciones $\mat{U}x=b$, con $\mat{U}$ triangular superior, y realiza el proceso recorriendo las filas de la matriz en orden inverso, desde abajo hacia arriba.

      \vspace*{1em}
      \begin{algorithm}[H]
        \SetKwFunction{sustAtras}{Sustitución hacia atrás}
        \caption{Sustitución hacia atrás}
        \KwData{$\mat{A} \in \mathbb{R}^{n \times n}, b \in \mathbb{R}^n$}
        \KwResult{$s \in \mathbb{R}^n$, solución del sistema $\mat{A}x=b$, considerando a $\mat{A}$ como una matriz triangular superior}
        \For{$i \gets n$ \KwTo $1$}{
          $suma \gets 0$ \;
          \For{$j \gets i + 1$ \KwTo $n$}{
            $suma \gets suma + s_j \cdot \mat{A}_{i,j}$ \;
          }
          $s_i \gets \frac{b_i - suma}{\mat{A}_{i,i}}$
        }
      \end{algorithm}
      \vspace*{1em}

    La idea central de los dos métodos que se utilizarán en este trabajo, \emph{Eliminación Gaussiana} y \emph{Factorización LU}, es la misma: llevar cualquier sistema de ecuaciones a otro equivalente en forma triangular, para así poder aplicar los algoritmos recién expuestos y encontrar su solución.

    \subsubsection{Eliminación Gaussiana}

      El método de \emph{Eliminación Gaussiana} itera sobre las columnas de la matriz, colocando ceros en todas las posiciones que se encuentran por debajo de la diagonal. Para hacer esto, en la $i$-ésima iteración, se resta a todas las filas a partir de la $i + 1$ un múltiplo de la fila $i$-ésima, con un factor elegido convenientemente. Esto asegura que, al completar el algoritmo, la matriz obtenida será triangular superior.

      En su forma más básica, el método falla si en alguna iteración se anula el elemento de la diagonal correspondiente a la columna sobre la que se está trabajando. Para salvar esta dificultad, se utiliza una técnica conocida como \emph{pivoteo}, que consiste en alterar el orden de las filas o de las columnas de la matriz. Sin embargo, como probaremos más adelante, la matriz asociada al sistema que estudiaremos tiene la particularidad de que el algoritmo de Eliminación Gaussiana puede aplicarse sin realizar pivoteo.

      \begin{algorithm}[H]
        \caption{Eliminación Gaussiana}
        \KwData{$A \in \mathbb{R}^{n \times n}, b \in \mathbb{R}^n$}
        \KwResult{$s \in \mathbb{R}^n$, solución del sistema $Ax=b$}
        \For{$j \gets 1$ \KwTo $n$}{
          \For{$i \gets 1$ \KwTo $n$}{
            $m \gets \frac{A_{i,j}}{A_{j,j}}$ \;
            \For{$k \gets j$ \KwTo $n$}{
              $A_{i,k} \gets A_{i,k} - m \cdot A_{j,k}$ \;
            }
            $b_i \gets b_i - m \cdot b_j$ \;
          }
        }
        $s \gets$ \sustAtras{$A,b$} \;
      \end{algorithm}

    \subsubsection{Factorización LU}

      El método de \emph{Factorización LU}, por su parte, aprovecha el hecho de que bajo ciertas condiciones, una matriz $A$ puede factorizarse como el producto de otras dos, en la forma $A = LU$, donde $L$ es triangular inferior con unos en la diagonal, y $U$ es triangular superior. De esta manera, el sistema $Ax=b$ puede reescribirse como $LUx=b$, y luego ser resuelto en dos etapas sencillas: si llamamos $y=Ux$, podemos resolver primero el sistema $Ly=b$ (aplicando sustitución hacia adelante, pues $L$ es triangular inferior), y una vez conocido el valor de $y$, resolver $Ux=y$ (aplicando esta vez sustitución hacia atrás), obteniendo así la solución del sistema original.

      \begin{algorithm}[H]
        \caption{Factorización LU}
        \KwData{$A \in \mathbb{R}^{n \times n}$}
        \KwResult{$B \in \mathbb{R}^{n \times n}$, tal que los elementos ubicados por encima de y en la diagonal forman una matriz triangular superior $U$, y los ubicados por debajo de la misma, completados con unos en la diagonal, forman una matriz triangular inferior $L$, con $LU=A$}
        \For{$j \gets 1$ \KwTo $n$}{
          \For{$i \gets 1$ \KwTo $n$}{
            $m \gets \frac{A_{i,j}}{A_{j,j}}$ \;
            $B_{i}{j} \gets m$ \;
            \For{$k \gets j+1$ \KwTo $n$}{
              $B_{i,k} \gets A_{i,k} - m \cdot A_{j,k}$ \;
            }
          }
        }
      \end{algorithm}

  \subsection{Modelado del sistema}

     Llamamos $r_e \in \mathbb{R}_{>0}$ al radio exterior y $r_i \in \mathbb{R}_{>0}$ al radio interior de la pared del horno. Para cada punto, consideramos sus coordenadas polares $(r, \theta)$ con origen en el centro del horno, y llamaremos $T(r, \theta)$ a la temperatura en dicho punto. En el estado estacionario, esta temperatura satisface la ecuación del calor:

      \begin{equation} \label{des:eq:calor}
        \frac{\partial^2 T(r, \theta)}{\partial r^2} + \frac{1}{r} \frac{\partial T(r, \theta)}{\partial r} + \frac{1}{r^2} \frac{\partial^2 T(r, \theta)}{\partial \theta^2} = 0
      \end{equation}

      Para poder tratar computacionalmente el problema, necesitamos una versión discreta del mismo. Con este fin, consideramos una partición del sector interno de la pared del horno en $m + 1$ radios equidistantes, $r_i = r_0 < r_1 < \dots < r_m = r_e$, con $r_j - r_{j-1} = \Delta r$ para $j = 1, \dots m+1$, y en $n$ ángulos, $0 = \theta_0 < \theta_1 < ... < \theta_n = 2\pi$, con $\theta_k - \theta_{k-1} = \Delta \theta$ para $k = 1, \dots n$.

      Nuestro objetivo es calcular el valor de $t_{j,k} = T(r_j, \theta_k)$ para $j = 0, \dots m$ y $k = 0, \dots n-1$, utilizando la ecuación (\ref{des:eq:calor}) y los datos disponibles sobre las condiciones de borde: conocemos el valor de la temperatura en los puntos de la pared externa, $t_{m,k} = T_e(\theta_k)$, para todo $k = 0, \dots n-1$, como así también el valor de la temperatura en el interior del horno, $t_{0,k} = T_i$, que es constante con respecto a $k$.

      Para discretizar el problema continuo planteado por la ecuación (\ref{des:eq:calor}), utilizamos las siguientes fórmulas de diferencias finitas:
      \[ \frac{\partial^2 T(r, \theta)}{\partial r^2}(r_j, \theta_k) \cong \frac{t_{j-1,k} - 2 t_{jk} + t_{j+1,k}}{(\Delta r)^2} \]
      \[ \frac{\partial T(r, \theta)}{\partial r}(r_j, \theta_k) \cong \frac{t_{j,k} - t_{j-1,k}}{\Delta r} \]
      \[ \frac{\partial^2 T(r, \theta)}{\partial \theta^2}(r_j, \theta_k) \cong \frac{t_{j,k-1} - 2 t_{jk} + t_{j,k+1}}{(\Delta \theta)^2} \]
      que nos permiten obtener un conjunto de ecuaciones lineales en las incógnitas $t_{j,k}$.

      De esta manera, podemos reunir la información de la que disponemos sobre el problema discreto en un sistema de $(m+1)n$ ecuaciones con $(m+1)n$ incógnitas, cada una de ellas el valor de $t_{j,k}$ para algún $j = 0, \dots m$, $k = 0, \dots n-1$. En otras palabras, el problema de calcular la temperatura en todos los puntos de la discretización se reduce a la resolución del siguiente sistema de ecuaciones lineales:

      \begin{equation} \label{des:eq:sistema}
        \begin{cases}
        \setlength{\jot}{2em}
          t_{0,k} = T_i
            & \text{para } 0 \leq k < n \\[1em]
          \begin{gathered}
            \left( \frac{1}{(\Delta r)^2} - \frac{1}{r \Delta r} \right) t_{j-1,k} +
              \left( \frac{1}{r^2(\Delta \theta)^2} \right) t_{j,k-1} + \\
              \left( - \frac{2}{(\Delta r)^2} + \frac{1}{r \Delta r} - \frac{2}{r^2(\Delta \theta)^2} \right) t_{j,k} + \\
              \left( \frac{1}{r^2(\Delta \theta)^2} \right) t_{j,k+1} +
              \left( \frac{1}{(\Delta r)^2} \right) t_{j+1,k} = 0 \\[1em]
            \end{gathered}
            & \text{para } 1 \leq j < m,\ 0 \leq k < n \\
          t_{m,k} = T_e(\theta_k)
            & \text{para } 0 \leq k < n \\
        \end{cases}
      \end{equation}

      Para construir la matriz $A \in \mathbb{R}^{(m+1)n \times (m+1)n}$ asociada a este sistema, ordenamos las ecuaciones y las variables de la siguiente forma:
      \begin{itemize}
        \item La ecuación referida a la variable $t_{j,k}$, es decir, la ecuación donde aparece como única incógnita cuando $j = 0$ o $j = m$, o bien la ecuación donde aparece junto a sus cuatro puntos vecinos en la discretización si $1 \leq j < m$, es representada por la fila $p$-ésima de la matriz, con $p = jn+k$.
        \item A la variable $t_{j,k}$ le corresponde la columna $q$-ésima, con $q = jn+k$.
      \end{itemize}

      La matriz $A$ construida de esta forma cumple con la importante propiedad de ser \emph{banda} $n, n$. Esto significa que todos los coeficientes que se encuentran fuera de una banda de ancho $n$ hacia ambos lados de la diagonal son nulos, i.e. $A_{p,q} = 0$ siempre que $\vert p - q \vert > n$. Esto puede verse fácilmente:

      \begin{itemize}
        \item Las filas $1, \dots, n$ y $mn + 1, \dots, (m+1)n$ de la matriz, que corresponden a las ecuaciones relativas a los puntos de la pared interna y externa del horno, respectivamente, contienen un $1$ en la diagonal y $0$ en todas las demás posiciones, por lo que la propiedad se verifica trivialmente.
        \item Para $p = n + 1, \dots, mn$, la fila $p$-ésima de la matriz representa a la ecuación relativa a $t_{j,k}$, con $p = jn+k$. Es sencillo comprobar que los coeficientes correspondientes a $t_{j-1,k}$, $t_{j,k-1}$, $t_{j,k}$, $t_{j,k+1}$ y $t_{j+1,k}$ se encontrarán en las columnas $p-n$, $p-1$, $p$, $p+1$ y $p+n$, respectivamente. Estos elementos están confinados a una banda de ancho $n$ hacia ambos lados de la diagonal, y como las variables a las que corresponden son las únicas que aparecen en la ecuación, todos los demás coeficientes de la fila serán necesariamente nulos.
      \end{itemize}

  \subsection{Aplicabilidad de los métodos elegidos}

    Los métodos numéricos ya presentados no son aplicables a la hora de resolver cualquier sistema de ecuaciones lineales, sino que requieren que este satisfaga determinadas hipótesis. Por este motivo, es necesario demostrar que estas se cumplen en el caso particular del sistema que modela el problema a resolver en este trabajo.

    \subsubsection{Preliminares}

        A continuación se enunciarán dos resultados generales previos que luego serán utilizados para la demostración general.

        \begin{prop} \label{prop:EG sin pivoteo implica LU}
          Si el sistema $Ax=b$ puede ser resuelto aplicando Eliminación Gaussiana sin pivoteo, entonces la matriz $A$ puede ser factorizada de la forma $A=LU$, donde $U$ es una matriz triangular superior y $L$ es una matriz triangular inferior con unos en la diagonal.
        \end{prop}
        \begin{proof}
            Puede encontrarse en \cite[p.~403]{burden}.
        \end{proof}

        \begin{lema}
          \label{lema:EG conserva diagonal dominante}
          Sea $A^{(0)} = A \in \mathbb{R}^{n \times n}$ una matriz diagonal dominante, con $A_{i,j} \neq 0$, y $A^{(1)}$ el resultado de aplicar un paso de Eliminación Gaussiana (sin pivoteo) sobre $A$. Entonces $A^{(1)}$ es diagonal dominante.
        \end{lema}
        \begin{proof}
          Consideremos la fila $i$-ésima de $A^{(1)}$, y veamos que $\left \vert A^{(1)}_{i,i} \right \vert \geq \sum_{\substack{j = 1 \\ j \neq i}}^n \left \vert A^{(1)}_{i,j} \right \vert$. Tenemos que
          \[ \left \vert A^{(1)}_{i,i} \right \vert = \left \vert A_{i,i} - \frac{A_{i,1}}{A_{1,1}} A_{1,i} \right \vert
            \qquad \text{y} \qquad
          \sum_{\substack{j = 1 \\ j \neq i}}^n \left \vert A^{(1)}_{i,j} \right \vert
            = \sum_{\substack{j = 2 \\ j \neq i}}^n \left \vert A_{i,j} - \frac{A_{i,1}}{A_{1,1}} A_{1,j} \right \vert \]

          Luego,

          \[ \begin{split}
            \sum_{\substack{j = 1 \\ j \neq i}}^n \left \vert A^{(1)}_{i,j} \right \vert
            &= \sum_{\substack{j = 2 \\ j \neq i}}^n \left \vert A_{i,j} - \frac{A_{i,1}}{A_{1,1}} A_{1,j} \right \vert \\
            &\leq \sum_{\substack{j = 2 \\ j \neq i}}^n \vert A_{i,j} \vert + \left \vert \frac{A_{i,1}}{A_{1,1}} \right \vert \sum_{\substack{j = 2 \\ j \neq i}}^n \vert A_{1,j} \vert \\
            &\leq \left( \vert A_{i,i} \vert - \vert A_{i,1} \vert \right) + \left \vert \frac{A_{i,1}}{A_{1,1}} \right \vert \left( \vert A_{1,1} \vert - \vert A_{1,i} \vert \right) \\
            &= \vert A_{i,i} \vert - \left \vert \frac{A_{i,1}}{A_{1,1}} \right \vert \vert A_{1,i} \vert \\
            &\leq \left \vert A_{i,i} -  \frac{A_{i,1}}{A_{1,1}} A_{1,i} \right \vert = \left \vert A^{(1)}_{i,i} \right \vert \qedhere
          \end{split} \]
        \end{proof}

    \subsubsection{Demostración de la aplicabilidad}

        El objetivo es demostrar que tanto el método de Eliminación Gaussiana (sin pivoteo) como el de Factorización LU son aplicables en el sistema estudiado. Sin embargo, de lo enunciado en la Proposición \ref{prop:EG sin pivoteo implica LU} se desprende que basta con probar la aplicabilidad del primero de ellos, lo cual se hará a continuación.

        Sea $Ax=b$ la representación matricial correspondiente al sistema expuesto en (\ref{des:eq:sistema}), con $A \in \mathbb{R}^{(m+1)n \times (m+1)n}$ y $b \in \mathbb{R}^{(m+1)n}$ construidos de la manera arriba explicada. Entonces, el sistema $Ax=b$ puede ser resuelto aplicando Eliminación Gaussiana sin pivoteo.

        La matriz $A \in \mathbb{R}^{(m+1)n \times (m+1)n}$, correspondiente al sistema expuesto en (\ref{des:eq:sistema}), es diagonal dominante, de manera no estricta.

        \begin{itemize}
          \item Para las filas $1$ a $n$ de la matriz, correspondientes al borde interno de la discretización, y $mn + 1 $ a $(m+1)n$, correspondientes al borde externo, tenemos que
            \[ A_{p,q} = \begin{cases}
              1 & \text{si $p = q$} \\
              0 & \text{si $p \neq q$}
            \end{cases} \]
          Por lo tanto,
            \[ \vert A_{p,p} \vert = 1 \geq 0 = \sum_{\substack{q=1 \\ q \neq p}}^{(m+1)n} \vert A_{p,q} \vert \]

          \item Las filas $n + 1$ a $mn$ de la matriz corresponden a los puntos $t_{j,k}$ de la discretización con $1 \leq j < m$. En todos estos casos, encontramos solo 5 coeficientes no nulos en la $p$-ésima fila, siendo el que aparece en la diagonal el correspondiente a la variable $t_{j,k}$, y por lo tanto
            \[ \begin{split}
              \vert A_{p,p} \vert &= \left \vert - \frac{2}{(\Delta r)^2} + \frac{1}{r \Delta r} - \frac{2}{r^2 (\Delta \theta)^2} \right \vert \\
              &= \left \vert - \frac{1}{(\Delta r)^2} + \frac{1}{r \Delta r} - \frac{2}{r^2 (\Delta \theta)^2} - \frac{1}{(\Delta r)^2} \right \vert \\
              &= \left \vert - \frac{r - \Delta r}{r (\Delta r)^2} - \frac{2}{r^2 (\Delta \theta)^2} - \frac{1}{(\Delta r)^2} \right \vert \\
              &= \left \vert \frac{r - \Delta r}{r (\Delta r)^2} + \frac{2}{r^2 (\Delta \theta)^2} + \frac{1}{(\Delta r)^2} \right \vert
            \end{split} \]

          Teniendo en cuenta que
            \[ r - \Delta r = (r_i + j \Delta r) - \Delta r = r_i + (j - 1) \Delta r > 0\]
          dado que $r_i, \Delta r > 0$ y $j \geq 1$, es fácil notar que todos los términos que aparecen dentro del módulo son positivos, y por lo tanto
            \[ \vert A_{p,p} \vert = \frac{r - \Delta r}{r (\Delta r)^2} + \frac{2}{r^2 (\Delta \theta)^2} + \frac{1}{(\Delta r)^2} \]

          Por otro lado
            \[ \begin{split}
              \sum_{\substack{q=1 \\ q \neq p}}^{(m+1)n} \vert A_{p,q} \vert &= \left \vert \frac{1}{(\Delta r)^2} - \frac{1}{r \Delta r} \right \vert + 2 \left \vert \frac{1}{r^2 (\Delta \theta)^2} \right \vert + \left \vert \frac{1}{(\Delta r)^2} \right \vert \\
              &= \left \vert \frac{r - \Delta r}{r (\Delta r)^2} \right \vert + \frac{2}{r^2 (\Delta \theta)^2} + \frac{1}{(\Delta r)^2} \\
              &= \frac{r - \Delta r}{r (\Delta r)^2} + \frac{2}{r^2 (\Delta \theta)^2} + \frac{1}{(\Delta r)^2} = \vert A_{p,p} \vert
            \end{split} \]
        \end{itemize}

        De lo anterior, se sigue que
          \[ \vert A_{p,p} \vert \geq \sum_{\substack{q=1 \\ q \neq p}}^{(m+1)n} \vert A_{p,q} \vert \qquad \text{para todo $p = 1, \dots, (m+1)n$} \]
        es decir, $A$ es diagonal dominante de forma no estricta.

        \begin{obs}
        \label{obs:Diagonal de A sin ceros}
        Para todo $p = 1, \dots, (m+1)n$, $\vert A_{p,p} \vert > 0$, i.e. la diagonal de $A$ no contiene ceros.
        \end{obs}

        Probaremos que para $u \leq (m+1)n$, es posible realizar $u$ iteraciones de Eliminación Gaussiana sobre $A$ sin pivoteo, y que luego de las mismas, la matriz $A^{(u)}$ resultante es diagonal dominante, con su fila $u + 1$ no nula. Lo haremos por inducción en $u$.

        \begin{itemize}
          \item[\textbf{C.B.}] Teniendo en cuenta que la primera fila de la matriz $A$ tiene necesariamente un $1$ en la diagonal y $0$ en las demás posiciones, se deduce trivialmente que es posible aplicar el primer paso de Eliminación Gaussiana sin realizar pivoteo, dado que $A_{1, 1}$ es no nulo. Del Lema \ref{lema:EG conserva diagonal dominante}, se sigue que la matriz $A^{(1)}$ obtenida será diagonal dominante. Como $A_{1, 2} = 0$, tendremos $A^{(1)}_{2,2} = A_{2,2}$, que es necesariamente no nulo (Observación \ref{obs:Diagonal de A sin ceros}), por lo que la segunda fila de $A^{(1)}$ será no nula.
              
          \item[\textbf{P.I.}] Supongamos que la matriz $A^{(u - 1)}$, obtenida tras aplicar $u - 1$ pasos de Eliminación Gaussiana sobre $A$, es diagonal dominante, con su $u$-ésima fila no nula. Esto implica que $A^{(u - 1)}_{u,u} \neq 0$. Notemos que podemos escribir a $A^{(u - 1)}$ por bloques de la siguiente manera
            \[ A^{(u - 1)} = \left( \begin{matrix} U_{u - 1} & B \\ 0 & \widetilde{A}_{u - 1} \end{matrix} \right) \]
          donde las matrices $U_{u - 1} \in \mathbb{R}^{(u-1)\times(u-1)}$ y $\widetilde{A}_{u - 1} \in \mathbb{R}^{(n-u+1)\times(n-u+1)}$ son trivialmente diagonal dominantes.

          Podemos ver que realizar el paso $u$-ésimo del algoritmo de Eliminación Gaussiana sobre $A$ equivale a realizar el primer paso de este algoritmo sobre $\widetilde{A}_{u - 1}$, sin modificar el resto de la matriz. Dado que $A^{(u - 1)}_{u,u} \neq 0$, podremos realizar esto sin pivoteo, y del Lema 1 se sigue que el resultado de este proceso será diagonal dominante, lo cual implica directamente que también lo será $A^{(u)}$.

          Por último, probemos que la fila $u + 1$ de la matriz $A^{(u)}$ es no nula.
          \begin{itemize}
            \item Si $1 \leq u + 1 \leq n$ o $mn < u + 1 \leq (m+1)n$, tendremos que la fila $u + 1$ tendrá un $1$ en la diagonal y $0$ en las demás posiciones, por lo que no se verá afectada por el algoritmo de Eliminación Gaussiana. Luego $A^{(u)}_{u+1,u+1} = 1$.
            \item En caso contrario, por la forma en la que se construye la matriz del sistema, $A_{u+1,u+1+n}$ corresponde al coeficiente de la variable $t_{j,k+1}$ de la $u + 1$-ésima ecuación del sistema, que es trivialmente no nulo. Además, como notamos anteriormente, la matriz $A$ es \emph{banda} $n, n$. Por lo tanto, $A_{p,u+1+n} = 0$ para todo $p = 0, \dots, u$ y, por consiguiente, $A^{(u)}_{u+1,u+1+n} = A_{u+1,u+1+n}$. Esto implica que la fila $u + 1$ de la matriz $A^{(u)}$ es no nula, como queríamos probar. \qedhere
          \end{itemize}
        \end{itemize}

  \subsection{Interpretación de los resultados}

    \subsubsection{Estimación de la posición de la isoterma crítica}

      Una vez obtenida la resolución del sistema, fue necesario decidir un criterio para estimar la ubicación de la isoterma pedida. Algunas de las alternativas que consideramos al respecto fueron las siguientes:

      \begin{itemize}
        \item Para cada ángulo $\theta_k$, considerar como posición de la isoterma el radio correspondiente al punto $(r_j, \theta_k)$ de la discretización con el valor de $t_{j,k}$ más próximo a 500{\degree}C.
        \item Para cada ángulo $\theta_k$, considerar como posición de la isoterma el radio del punto de la discretización inmediatamente exterior al primero con temperatura mayor o igual a 500{\degree}C, contando desde la pared externa. Es decir, considerar el menor radio $r_j$ tal que $t_{j',k} < 500$ para todo $j' \geq j$. La ventaja de este método es que asegura que la isoterma nunca se encontrará más cerca de la pared externa que el resultado arrojado, garantizando que se detectarán todas las posibles situaciones de riesgo.
        \item Para cada ángulo $\theta_k$, considerar $r_j$, el radio del punto más externo de la discretización con temperatura mayor o igual a 500{\degree}C. Luego, sabiendo que la isoterma buscada se encontrará entre este radio y el inmediatamente exterior, aproximar la variación de la temperatura en los puntos intermedios mediante una función lineal y utilizar esta aproximación para estimar la posición de la isoterma. Más precisamente, si $r_{iso}$ es el radio de la isoterma buscada, entonces
          \[ r_{iso} = r_j + \Delta r \left(\frac{500 - t_{j,k}}{t_{j+1,k} - t_{j,k}} \right) \]
      \end{itemize}

      Finalmente, decidimos aplicar este último criterio, porque consideramos que brinda información más precisa que los otros dos, al tener en cuenta el valor la diferencia de temperatura entre los puntos de la discretización más cercanos a la isoterma buscada y la isoterma en si misma, información que los dos primeros métodos descartan. De esta manera, nos permite detectar situaciones de riesgo que con el primer método pasarían desapercibidas, pero evitando la generación de falsos positivos que provocaría el segundo método, que es mucho más estricto.

    \subsubsection{Cálculo del índice de peligrosidad}

      Para elegir la medida de peligrosidad lo que hacemos es tomar de todos los valores por los que pasa la isoterma el mas próximo a la pared externa del alto horno. A ese valor lo dividimos por el radio externo, dando así la relación entre la distancia mas cercana a la pared externa de la isoterma y la distancia total.
      Este es un numero entre 1 y 0 donde 0 es el centro del horno y 1 es la pared externa. Cuanto mas cercano a 1 sea el valor, mas peligro de colapsar tiene. 

  \subsection{Implementación}

    Los dos métodos utilizados fueron implementados en lenguaje \texttt{C++}. (...)