\section{Desarrollo}

    \subsection{Conceptos teóricos} \label{subsec:conceptos-teoricos}

        La resolución de \emph{sistemas de ecuaciones lineales} es un problema recurrente en el análisis numérico, ya que estos son útiles a la hora de modelar matemáticamente el comportamiento de problemas provenientes de diversas disciplinas, como la física y la ingeniería, para ser tratados en forma computacional. Es por eso que resulta de interés desarrollar algoritmos que permitan, de manera tan eficiente como sea posible, encontrar soluciones a estos sistemas.

        Un sistema de $m$ ecuaciones lineales con $n$ incógnitas tiene la forma
        \[ \left\lbrace \begin{matrix}
                a_{1,1} \, x_1 & + & a_{1,2} \, x_2 & + & \dots  & + & a_{1,n} \, x_n & = & b_1    \\
                a_{2,1} \, x_1 & + & a_{2,2} \, x_2 & + & \dots  & + & a_{2,n} \, x_n & = & b_2    \\
                \vdots         &   & \vdots         &   & \vdots &   & \vdots         &   & \vdots \\
                a_{m,1} \, x_1 & + & a_{m,2} \, x_2 & + & \dots  & + & a_{m,n} \, x_n & = & b_m    \\
        \end{matrix} \right. \]
        donde $a_{j,k}$ y $b_k$ (para $j = 1, \dots, m$; $k = 1, \dots, n$) son constantes reales. Por \emph{resolver} el sistema se entiende hallar un conjunto de valores para las incógnitas $x_1, \dots, x_n$ de forma que todas las ecuaciones se satisfagan simultáneamente.

        Una manera frecuente de representar un sistema de ecuaciones lineales, que será utilizada en este trabajo, es la matricial, que consiste en expresarlo en la forma
        \[ \mat{A}x = b \]
        donde 
        \[ \mat{A} = \left[ \begin{matrix} a_{1,1} & a_{1,2} & \dots  & a_{1,n} \\
                                           a_{2,1} & a_{2,2} & \dots  & a_{2,n} \\
                                           \vdots  & \vdots  & \ddots & \vdots  \\
                                           a_{m,1} & a_{m,2} & \dots  & a_{m,n} \\ \end{matrix} \right] \in \mathbb{R}^{m \times n} \qquad
        x = \left[ \begin{matrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{matrix} \right] \in \mathbb{R}^n \qquad
        b = \left[ \begin{matrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{matrix} \right] \in \mathbb{R}^m \]

        Dado un sistema de ecuaciones lineales, existe un conjunto de operaciones que, aplicadas sobre las ecuaciones, generan sistemas \emph{equivalentes}, es decir, que tienen las mismas soluciones. Más especificamente, estas operaciones son:
        \begin{enumerate}[label=(\alph*), nosep]
            \item Multiplicar una ecuación por una constante $\lambda \in \mathbb{R}$ no nula.
            \item Sumar a una ecuación el resultado de multiplicar otra por una constante $\lambda \in \mathbb{R}$.
            \item Intercambiar el orden de dos ecuaciones.
        \end{enumerate}

        Lo mismo vale para sistemas en forma matricial, si se consideran las operaciones análogas entre las filas de la matriz.

        Más aún, puede probarse que a partir de todo sistema puede obtenerse otro equivalente que se encuentra en forma \emph{triangular} o \emph{reducida} (ver \cite[p.~358]{burden}). Este resultado es importante porque, dado un sistema en forma triangular (superior o inferior), encontrar sus soluciones es sumamente sencillo, utilizando los algoritmos de \emph{sustitución hacia adelante} y el de \emph{sustitución hacia atrás}.

        Cabe destacar que un sistema de ecuaciones lineales puede no tener soluciones, tener solución única, o tener infinitas soluciones. En adelante se considerarán únicamente sistemas cuadrados (con igual número de ecuaciones que de incógnitas) cuya solución sea única, quedando los demás casos fuera del alcance de este trabajo. Bajo esta hipótesis, un sistema $\mat{A}x=b$ cumple la importante propiedad de que la matriz $\mat{A}$ es \emph{inversible}. En el caso de que $\mat{A}$ se encuentre en forma triangular, esto equivale a decir que no hay elementos nulos en su diagonal.

        El algoritmo de \emph{sustitución hacia adelante} considera un sistema de ecuaciones $\mat{L}x=b$, donde $\mat{L}$ es una matriz triangular inferior, y devuelve un vector $s$ solución del sistema. Lo hace aprovechando el hecho de, dado que la primer ecuación depende solamente de la primera incógnita, el valor de esta última puede ser despejado de forma inmediata; luego, sustituyendo este valor en la segunda ecuación, puede despejarse la segunda incógnita, y así sucesivamente hasta resolver el sistema completo. El algoritmo de \emph{sustitución hacia atrás} es análogo al anterior, pero considera un sistema de ecuaciones $\mat{U}x=b$, con $\mat{U}$ triangular superior, y realiza el proceso recorriendo las filas de la matriz en orden inverso, desde abajo hacia arriba. Ambos algoritmos tienen una complejidad cuadrática en la cantidad de incógnitas.

        \vspace{.5em}
        \begin{algorithm}[H]
            \caption{Sustitución hacia adelante} \label{alg:forward-substitution}
            \SetKwFunction{sustAdelante}{Sustitución hacia adelante}
            \KwData{$\mat{A} \in \mathbb{R}^{n \times n}, b \in \mathbb{R}^n$}
            \KwResult{$s \in \mathbb{R}^n$, solución del sistema $\mat{A}x=b$, considerando a $\mat{A}$ como una matriz triangular inferior, sin elementos nulos en la diagonal}
            \For{$j \gets 1$ \KwTo $n$}{
                $suma \gets 0$ \;
                \For{$k \gets 1$ \KwTo $j$}{
                    $suma \gets suma + s_k \cdot a_{j,k}$ \;
                }
                $s_j \gets \frac{b_j - suma}{a_{j,j}}$
            }
        \end{algorithm}
        \vspace{.5em}

        \vspace{.5em}
        \begin{algorithm}[H]
            \SetKwFunction{sustAtras}{Sustitución hacia atrás}
            \caption{Sustitución hacia atrás} \label{alg:backward-substitution}
            \KwData{$\mat{A} \in \mathbb{R}^{n \times n}, b \in \mathbb{R}^n$}
            \KwResult{$s \in \mathbb{R}^n$, solución del sistema $\mat{A}x=b$, considerando a $\mat{A}$ como una matriz triangular superior, sin elementos nulos en la diagonal}
            \For{$j \gets n$ \KwTo $1$}{
                $suma \gets 0$ \;
                \For{$k \gets j + 1$ \KwTo $n$}{
                    $suma \gets suma + s_k \cdot a_{j,k}$ \;
                }
                $s_j \gets \frac{b_j - suma}{a_{j,j}}$
            }
        \end{algorithm}
        \vspace{.5em}

        La sencillez con la que pueden resolverse sistemas cuando se encuentran en forma triangular es el principio en que se cimientan los dos métodos que se utilizarán en este trabajo, \emph{Eliminación Gaussiana} y \emph{Factorización LU}. La idea central de ambos es llevar el sistema de ecuaciones que pretende resolverse a otro equivalente en forma triangular, para así poder aplicar los algoritmos recién expuestos y encontrar su solución.

        \subsubsection{Eliminación Gaussiana}

            El método de \emph{Eliminación Gaussiana} itera sobre las columnas de la matriz, colocando ceros en todas las posiciones que se encuentran por debajo de la diagonal. Para hacer esto, en la $k$-ésima iteración, se resta a todas las filas a partir de la $k + 1$ un múltiplo de la fila $k$-ésima, con un factor elegido convenientemente.

            Más formalmente, supongamos que se pretende resolver el sistema $\mat{A}x = b$. Con la notación $\mat{A}^{(k)}$ se hará referencia a la matriz que se obtiene luego de aplicar $k$ iteraciones del algoritmo sobre $\mat{A}$, mientras que $\mat{A}_{k,*}$ denotará la $k$-ésima fila de la matriz $\mat{A}$.

            Inicialmente, se tiene que $\mat{A}^{(0)} = \mat{A}$. Luego, tras cada iteración, se obtiene:
            \[ \mat{A}^{(k)} = \left[ \begin{matrix}
                \mat{A}^{(k-1)}_{1,*} \\
                \vdots \\
                \mat{A}^{(k-1)}_{k,*} \\
                \mat{A}^{(k-1)}_{k+1,*} - m_{k+1,k} \mat{A}^{(k-1)}_{k,*} \\
                \vdots \\
                \mat{A}^{(k-1)}_{n,*} - m_{n,k} \mat{A}^{(k-1)}_{k,*} \\
            \end{matrix} \right] \]
            donde $m_{i,k} = \frac{a_{i,k}}{a_{k,k}}$. Esta elección de los multiplicadores tiene la particularidad de anular todos los elementos de la columna $k$ por debajo de la diagonal. Esto asegura que, al completar el algoritmo, luego de iterar sobre todas las columnas, la matriz obtenida será triangular superior.

            En su forma más básica, el método falla si, al comienzo de alguna iteración, es nulo el elemento de la diagonal correspondiente a la columna sobre la que se debe trabajar, es decir, si para algún $k = 1, \dots, n$ se cumple $a^{(k-1)}_{k,k} = 0$. Para salvar esta dificultad, se utiliza una técnica conocida como \emph{pivoteo}, que consiste en alterar el orden de las filas o de las columnas de la matriz. No obstante, bajo determinadas hipótesis, puede garantizarse que el algoritmo es aplicable sin necesidad de pivoteo; este será el caso en el contexto particular de este trabajo, por lo que no se entrará en más detalles acerca de esta técnica.

            \vspace{.5em}
            \begin{algorithm}[H]
                \caption{Eliminación Gaussiana} \label{alg:gaussian-elimination}
                \KwData{$\mat{A} \in \mathbb{R}^{n \times n}, b \in \mathbb{R}^n$}
                \KwResult{$s \in \mathbb{R}^n$, solución del sistema $\mat{A}x=b$}
                \For{$k \gets 1$ \KwTo $n$}{
                    \For{$j \gets k+1$ \KwTo $n$}{
                        $m \gets \frac{a_{j,k}}{a_{k,k}}$ \;
                        \For{$\ell \gets k$ \KwTo $n$}{
                            $a_{j,\ell} \gets a_{j,\ell} - m \cdot a_{k,\ell}$ \;
                        }
                        $b_j \gets b_j - m \cdot b_k$ \;
                    }
                }
                $s \gets$ \sustAtras{$\mat{A},b$} \;
            \end{algorithm}
        \vspace{.5em}

        \subsubsection{Factorización LU}

            El método de \emph{Factorización LU}, por su parte, aprovecha el hecho de que bajo ciertas condiciones, una matriz $\mat{A}$ puede factorizarse como el producto de otras dos, en la forma $\mat{A} = \mat{L}\mat{U}$, donde $\mat{L}$ es triangular inferior con unos en la diagonal, y $\mat{U}$ es triangular superior. De esta manera, el sistema $\mat{A}x=b$ puede reescribirse como $\mat{L}\mat{U}x=b$, y luego ser resuelto en dos etapas sencillas: si llamamos $y=\mat{U}x$, podemos resolver primero el sistema $\mat{L}y=b$ (aplicando sustitución hacia adelante, pues $\mat{L}$ es triangular inferior), y una vez conocido el valor de $y$, resolver $\mat{U}x=y$ (aplicando esta vez sustitución hacia atrás), obteniendo así la solución del sistema original.

            El pseudocódigo que se presenta como Algoritmo \ref{alg:lu-decomposition} muestra el procedimiento a seguir para encontrar la factorización LU de una matriz, asumiendo que dicha factorización existe. El mismo genera como salida una única matriz $\mat{B}$ que contiene tanto los coeficientes no nulos de $\mat{L}$ como los de $\mat{U}$; esta es una optimización destinada a la implementación, con el objetivo de mejorar su eficiencia evitando el almacenamiento de información redundante.

            {\color{red}
                IMPORTANTE: REVISAR EL PSEUDOCÓDIGO, PORQUE CREO QUE ESTÁ MAL
            }

            \vspace{.5em}
            \begin{algorithm}[H]
                \caption{Factorización LU} \label{alg:lu-decomposition}
                \KwData{$\mat{A} \in \mathbb{R}^{n \times n}$}
                \KwResult{$\mat{B} \in \mathbb{R}^{n \times n}$, tal que los elementos ubicados por encima de y en la diagonal forman una matriz triangular superior $\mat{U}$, y los ubicados por debajo de la misma, completados con unos en la diagonal, forman una matriz triangular inferior $\mat{L}$, con $\mat{L}\mat{U}=\mat{A}$}
                \For{$k \gets 1$ \KwTo $n$}{
                    \For{$j \gets k+1$ \KwTo $n$}{
                        $m \gets \frac{a_{j,k}}{a_{k,k}}$ \;
                        $b_{j,k} \gets m$ \;
                        \For{$\ell \gets k+1$ \KwTo $n$}{
                            $b_{j,\ell} \gets a_{j,\ell} - m \cdot a_{k,\ell}$ \;
                        }
                    }
                }
            \end{algorithm}
            \vspace{.5em}

            En cuanto a las hipótesis que permiten asegurar la existencia de la factorización LU de una matriz, resultará de utilidad el siguiente resultado, cuya demostración puede encontrarse en \cite[p.~403]{burden}.

            \begin{prop} \label{prop:EG sin pivoteo implica LU}
                Si una matriz $\mat{A} \in \mathbb{R}^{n \times n}$ es tal que el sistema $\mat{A}x=b$ ($b \in \mathbb{R}^{n}$) puede ser resuelto aplicando Eliminación Gaussiana sin pivoteo, entonces $\mat{A}$ puede ser factorizada de la forma $\mat{A}=\mat{L}\mat{U}$, donde $\mat{U}$ es una matriz triangular superior y $\mat{L}$ es una matriz triangular inferior con unos en la diagonal.
            \end{prop}

    \subsection{Modelado del sistema}

        Para resolver el problema presentado en la Introducción, se considerará una sección horizontal del horno, como puede verse en la Figura \ref{fig:seccion-horno}. Se denominará $r_e \in \mathbb{R}_{>0}$ al radio exterior y $r_i \in \mathbb{R}_{>0}$ al radio interior de la pared del horno. Para cada punto, se considerarán sus coordenadas polares $(r, \theta)$ con origen en el centro del horno, llamando $T(r, \theta)$ a la temperatura en dicho punto. Se considerará conocido el valor (constante) de la temperatura en el interior del horno, \[ T_i = T(r, \theta) \qquad (\forall \, r \in [0, r_i], \, \theta \in [0, 2 \pi)) \] como así también el correspondiente a los puntos de la pared externa, \[ T_e(\theta) = T(r_e, \theta) \]
        
        \begin{figure}[h]
          \centering

          \includegraphics{imagenes/seccion-horno.pdf}

          \caption{Sección horizontal del horno.}
          \label{fig:seccion-horno}
        \end{figure}

        En el estado estacionario, la temperatura $T$ satisface la ecuación del calor:
        \begin{equation} \label{des:eq:calor}
            \frac{\partial^2 T(r, \theta)}{\partial r^2} + \frac{1}{r} \frac{\partial T(r, \theta)}{\partial r} + \frac{1}{r^2} \frac{\partial^2 T(r, \theta)}{\partial \theta^2} = 0
        \end{equation}

        No obstante, esta ecuación plantea un modelo continuo para estudiar el problema. Para poder tratar computacionalmente el problema, se hace necesario discretizar su dominio, considerando una cantidad finita de puntos del mismo. Con este fin, se adoptará una partición del sector interno de la pared del horno, tal y como ilustra la Figura \ref{fig:discretizacion}, en $m + 1$ radios equidistantes 
        \[ r_i = r_0 < r_1 < \dots < r_m = r_e \text{, \qquad con } r_j - r_{j-1} = \Delta r \text{ para } j = 1, \dots, m+1 \]
        y en $n$ ángulos iguales
        \[ 0 = \theta_0 < \theta_1 < ... < \theta_n = 2\pi \text{, \qquad con } \theta_k - \theta_{k-1} = \Delta \theta \text{ para } k = 1, \dots, n \]

        \begin{figure}[h]
          \centering

          \includegraphics{imagenes/discretizacion.pdf}

          \caption{Discretización del dominio del problema.}
          \label{fig:discretizacion}
        \end{figure}

        Nuestro objetivo es calcular el valor de la temperatura en todos los puntos de la discretización $t_{j,k} = T(r_j, \theta_k)$ para $j = 0, \dots, m$ y $k = 0, \dots, n-1$. Gracias a los datos disponibles sobre las condiciones de borde, conocemos el valor de la temperatura en los puntos de las paredes externa e interna: $t_{m,k} = T_e(\theta_k)$ y $t_{0,k} = T_i$, para todo $k = 0, \dots, n-1$.

        Para discretizar el problema continuo planteado por la ecuación (\ref{des:eq:calor}), y así obtener el valor en los puntos internos de la pared, pueden utilizarse las siguientes fórmulas de diferencias finitas
        \begin{align*}
            \frac{\partial^2 T(r, \theta)}{\partial r^2}(r_j, \theta_k) &\cong \frac{t_{j-1,k} - 2 t_{jk} + t_{j+1,k}}{(\Delta r)^2} \\
            \frac{\partial T(r, \theta)}{\partial r}(r_j, \theta_k) &\cong \frac{t_{j,k} - t_{j-1,k}}{\Delta r} \\
            \frac{\partial^2 T(r, \theta)}{\partial \theta^2}(r_j, \theta_k) &\cong \frac{t_{j,k-1} - 2 t_{jk} + t_{j,k+1}}{(\Delta \theta)^2} 
        \end{align*}
        que, reemplazadas en la ecuación (\ref{des:eq:calor}), permiten obtener una formulación de la temperatura en estos puntos en función de sus puntos vecinos. Más específicamente, para $1 \leq j < m,\ 0 \leq k < n$, se tiene que $t_{j,k} = \Phi(j,k)$, donde
        \begin{multline*}
            \Phi(j,k) = \left( \frac{1}{(\Delta r)^2} - \frac{1}{r \Delta r} \right) t_{j-1,k} +
                \left( \frac{1}{r^2(\Delta \theta)^2} \right) t_{j,k-1} + \\
                \left( - \frac{2}{(\Delta r)^2} + \frac{1}{r \Delta r} - \frac{2}{r^2(\Delta \theta)^2} \right) t_{j,k} +
                \left( \frac{1}{r^2(\Delta \theta)^2} \right) t_{j,k+1} +
                \left( \frac{1}{(\Delta r)^2} \right) t_{j+1,k} = 0
        \end{multline*}

        De esta manera, podemos reunir la información de la que disponemos sobre el problema discreto en un sistema de $(m+1)n$ ecuaciones con $(m+1)n$ incógnitas, cada una de ellas el valor de la temperatura en un punto de la discretización, es decir, $t_{j,k}$ para algún $j = 0, \dots, m$, $k = 0, \dots, n-1$. En otras palabras, el problema de calcular la temperatura en todos los puntos de la discretización se traduce en la resolución del siguiente sistema de ecuaciones lineales:

        \begin{equation} \label{des:eq:sistema}
            \begin{cases}
                t_{0,k} = T_i
                    & \text{para } 0 \leq k < n \\
                t_{j,k} = \Phi(j,k)
                    & \text{para } 1 \leq j < m,\ 0 \leq k < n \\
                t_{m,k} = T_e(\theta_k)
                    & \text{para } 0 \leq k < n
            \end{cases}
        \end{equation}

        Como puede verse, cada ecuación del sistema está asociada directamente al cálculo de la temperatura en uno de los puntos de la discretización, es decir, a una de las incógnitas. Al escribir el sistema en forma matricial, se ordenan tanto las incógnitas $t_{j,k}$ como las ecuaciones asociadas a ellas, primero de acuerdo al radio ($j$) y luego de acuerdo al ángulo ($k$). Esto significa que los coeficientes correspondientes a la incógnita $t_{j,k}$ se encontrarán en la columna $(m+1)j + k$, y a la ecuación asociada al cálculo de esta incógnita le corresponderá la fila con el mismo índice.

        Puede notarse que las primeras $n$ filas y las últimas $n$ filas de la matriz, correspondientes a los puntos de la pared interna y externa del horno, respectivamente, contendrán un $1$ en la diagonal y $0$ en todas las demás posiciones, dado que allí la temperatura ya es conocida y no depende de ningún otro punto. En las demás filas, correspondientes a los puntos internos de la discretización, la temperatura depende solamente del valor de la misma en los cuatro vecinos más próximos; esto provoca que los coeficientes no nulos se encuentren confinados a un entorno relativamente reducido de la diagonal de la matriz. Más precisamente, todos los coeficientes que se encuentran fuera de una banda de ancho $n$ hacia ambos lados de la diagonal son nulos, i.e. $A_{p,q} = 0$ siempre que $\vert p - q \vert > n$. En otras palabras, la matriz construida es \emph{banda} $n, n$; este hecho puede observarse claramente en la Figura \ref{fig:matriz-banda}.

        \begin{figure}[h]
          \centering

          \includegraphics{imagenes/matriz-banda.pdf}

          \caption{Matriz asociada al sistema con $m = 4$ y $n = 10$. Aparecen resaltados los coeficientes no nulos.}
          \label{fig:matriz-banda}
        \end{figure}

    \subsection{Aplicabilidad de los métodos elegidos}

        Los métodos numéricos ya presentados no son aplicables a la hora de resolver cualquier sistema de ecuaciones lineales, sino que requieren que este satisfaga determinadas hipótesis. Por este motivo, es necesario demostrar que estas se cumplen en el caso particular del sistema que modela el problema a resolver en este trabajo.

        El objetivo es demostrar que tanto el método de Eliminación Gaussiana (sin pivoteo) como el de Factorización LU pueden aplicarse para resolver el sistema planteado en la sección anterior. Sin embargo, de lo enunciado en la Proposición \ref{prop:EG sin pivoteo implica LU} (sección \ref{subsec:conceptos-teoricos}) se desprende que basta con probar la aplicabilidad del primero de ellos. A continuación se procederá a introducir una definición y demostrar un lema, que luego se utilizarán durante la demostración.

        \begin{defi}
            Sea $\mat{A} \in \mathbb{R}^{n \times n}$. $\mat{A}$ se dice \emph{diagonal dominante} (por filas, de manera no estricta) si cada uno de los elementos de su diagonal es mayor o igual, en módulo, que la suma de los módulos de todos los demás coeficientes presentes en la misma fila. Es decir, si
            \[ \vert a_{j,j} \vert \geq \sum_{\substack{k=1 \\ k \neq j}}^n \vert a_{j,k} \vert \qquad \text{para todo $j = 1, \dots, n$} \]
        \end{defi}

        \begin{lema}
            \label{lema:EG conserva diagonal dominante}
            Sea $\mat{A}^{(0)} = \mat{A} \in \mathbb{R}^{n \times n}$ una matriz diagonal dominante, con $\mat{A}_{1,1} \neq 0$, y $\mat{A}^{(1)}$ el resultado de aplicar un paso de Eliminación Gaussiana (sin pivoteo) sobre $\mat{A}$. Entonces $\mat{A}^{(1)}$ es diagonal dominante.
        \end{lema}
        \begin{proof}
            Consideremos la fila $j$-ésima de $\mat{A}^{(1)}$, y veamos que $\left \vert a^{(1)}_{j,j} \right \vert \geq \sum_{\substack{k = 1 \\ k \neq j}}^n \left \vert a^{(1)}_{j,k} \right \vert$. Tenemos que
            \[ \left \vert a^{(1)}_{j,j} \right \vert = \left \vert a_{j,j} - \frac{a_{j,1}}{a_{1,1}} a_{1,j} \right \vert
                \qquad \text{y} \qquad
            \sum_{\substack{k = 1 \\ k \neq j}}^n \left \vert a^{(1)}_{j,k} \right \vert
                = \sum_{\substack{k = 2 \\ k \neq j}}^n \left \vert a_{j,k} - \frac{a_{j,1}}{a_{1,1}} a_{1,k} \right \vert \]

            Luego,

            \[ \begin{split}
                \sum_{\substack{k = 1 \\ k \neq j}}^n \left \vert a^{(1)}_{j,k} \right \vert
                &= \sum_{\substack{k = 2 \\ k \neq j}}^n \left \vert a_{j,k} - \frac{a_{j,1}}{a_{1,1}} a_{1,k} \right \vert \\
                &\leq \sum_{\substack{k = 2 \\ k \neq j}}^n \vert a_{j,k} \vert + \left \vert \frac{a_{j,1}}{a_{1,1}} \right \vert \sum_{\substack{k = 2 \\ k \neq j}}^n \vert a_{1,j} \vert \\
                &\leq \left( \vert a_{j,j} \vert - \vert a_{j,1} \vert \right) + \left \vert \frac{a_{j,1}}{a_{1,1}} \right \vert \left( \vert a_{1,1} \vert - \vert a_{1,j} \vert \right) \\
                &= \vert a_{j,j} \vert - \left \vert \frac{a_{j,1}}{a_{1,1}} \right \vert \vert a_{1,j} \vert \\
                &\leq \left \vert a_{j,j} -  \frac{a_{j,1}}{a_{1,1}} a_{1,j} \right \vert = \left \vert a^{(1)}_{j,j} \right \vert \qedhere
            \end{split} \]
        \end{proof}

        Contando con estos resultados, se probará que el sistema construido en la sección anterior, tal y como fue expuesto en (\ref{des:eq:sistema}), puede ser resuelto aplicando Eliminación Gaussiana sin pivoteo. Consideraremos la representación matricial del sistema $\mat{A}x=b$, con $\mat{A} \in \mathbb{R}^{(m+1)n \times (m+1)n}$ y $b \in \mathbb{R}^{(m+1)n}$. La demostración tendrá dos etapas:
        \begin{enumerate}[label=(\arabic*), nosep]
            \item Demostrar que la matriz $\mat{A}$ es diagonal dominante.
            \item Demostrar, a partir de este hecho y en forma inductiva, que todas las iteraciones del algoritmo de Eliminación Gaussiana están bien definidas, sin utilizar pivoteo.
        \end{enumerate}

        {\color{red}
            Hasta acá llegué
        }




            \begin{itemize}
                \item Para las filas $1$ a $n$ de la matriz, correspondientes al borde interno de la discretización, y $mn + 1 $ a $(m+1)n$, correspondientes al borde externo, tenemos que
                    \[ a_{p,q} = \begin{cases}
                        1 & \text{si $p = q$} \\
                        0 & \text{si $p \neq q$}
                    \end{cases} \]
                Por lo tanto,
                    \[ \vert A_{p,p} \vert = 1 \geq 0 = \sum_{\substack{q=1 \\ q \neq p}}^{(m+1)n} \vert A_{p,q} \vert \]

                \item Las filas $n + 1$ a $mn$ de la matriz corresponden a los puntos $t_{j,k}$ de la discretización con $1 \leq j < m$. En todos estos casos, encontramos solo 5 coeficientes no nulos en la $p$-ésima fila, siendo el que aparece en la diagonal el correspondiente a la variable $t_{j,k}$, y por lo tanto
                    \[ \begin{split}
                        \vert A_{p,p} \vert &= \left \vert - \frac{2}{(\Delta r)^2} + \frac{1}{r \Delta r} - \frac{2}{r^2 (\Delta \theta)^2} \right \vert \\
                        &= \left \vert - \frac{1}{(\Delta r)^2} + \frac{1}{r \Delta r} - \frac{2}{r^2 (\Delta \theta)^2} - \frac{1}{(\Delta r)^2} \right \vert \\
                        &= \left \vert - \frac{r - \Delta r}{r (\Delta r)^2} - \frac{2}{r^2 (\Delta \theta)^2} - \frac{1}{(\Delta r)^2} \right \vert \\
                        &= \left \vert \frac{r - \Delta r}{r (\Delta r)^2} + \frac{2}{r^2 (\Delta \theta)^2} + \frac{1}{(\Delta r)^2} \right \vert
                    \end{split} \]

                Teniendo en cuenta que
                    \[ r - \Delta r = (r_i + j \Delta r) - \Delta r = r_i + (j - 1) \Delta r > 0\]
                dado que $r_i, \Delta r > 0$ y $j \geq 1$, es fácil notar que todos los términos que aparecen dentro del módulo son positivos, y por lo tanto
                    \[ \vert A_{p,p} \vert = \frac{r - \Delta r}{r (\Delta r)^2} + \frac{2}{r^2 (\Delta \theta)^2} + \frac{1}{(\Delta r)^2} \]

                Por otro lado
                    \[ \begin{split}
                        \sum_{\substack{q=1 \\ q \neq p}}^{(m+1)n} \vert A_{p,q} \vert &= \left \vert \frac{1}{(\Delta r)^2} - \frac{1}{r \Delta r} \right \vert + 2 \left \vert \frac{1}{r^2 (\Delta \theta)^2} \right \vert + \left \vert \frac{1}{(\Delta r)^2} \right \vert \\
                        &= \left \vert \frac{r - \Delta r}{r (\Delta r)^2} \right \vert + \frac{2}{r^2 (\Delta \theta)^2} + \frac{1}{(\Delta r)^2} \\
                        &= \frac{r - \Delta r}{r (\Delta r)^2} + \frac{2}{r^2 (\Delta \theta)^2} + \frac{1}{(\Delta r)^2} = \vert A_{p,p} \vert
                    \end{split} \]
            \end{itemize}

            De lo anterior, se sigue que
                \[ \vert A_{p,p} \vert \geq \sum_{\substack{q=1 \\ q \neq p}}^{(m+1)n} \vert A_{p,q} \vert \qquad \text{para todo $p = 1, \dots, (m+1)n$} \]
            es decir, $A$ es diagonal dominante de forma no estricta.

            \begin{obs}
            \label{obs:Diagonal de A sin ceros}
            Para todo $p = 1, \dots, (m+1)n$, $\vert A_{p,p} \vert > 0$, i.e. la diagonal de $A$ no contiene ceros.
            \end{obs}

            Probaremos que para $u \leq (m+1)n$, es posible realizar $u$ iteraciones de Eliminación Gaussiana sobre $A$ sin pivoteo, y que luego de las mismas, la matriz $A^{(u)}$ resultante es diagonal dominante, con su fila $u + 1$ no nula. Lo haremos por inducción en $u$.

            \begin{itemize}
                \item[\textbf{C.B.}] Teniendo en cuenta que la primera fila de la matriz $A$ tiene necesariamente un $1$ en la diagonal y $0$ en las demás posiciones, se deduce trivialmente que es posible aplicar el primer paso de Eliminación Gaussiana sin realizar pivoteo, dado que $A_{1, 1}$ es no nulo. Del Lema \ref{lema:EG conserva diagonal dominante}, se sigue que la matriz $A^{(1)}$ obtenida será diagonal dominante. Como $A_{1, 2} = 0$, tendremos $A^{(1)}_{2,2} = A_{2,2}$, que es necesariamente no nulo (Observación \ref{obs:Diagonal de A sin ceros}), por lo que la segunda fila de $A^{(1)}$ será no nula.
                        
                \item[\textbf{P.I.}] Supongamos que la matriz $A^{(u - 1)}$, obtenida tras aplicar $u - 1$ pasos de Eliminación Gaussiana sobre $A$, es diagonal dominante, con su $u$-ésima fila no nula. Esto implica que $A^{(u - 1)}_{u,u} \neq 0$. Notemos que podemos escribir a $A^{(u - 1)}$ por bloques de la siguiente manera
                    \[ A^{(u - 1)} = \left( \begin{matrix} U_{u - 1} & B \\ 0 & \widetilde{A}_{u - 1} \end{matrix} \right) \]
                donde las matrices $U_{u - 1} \in \mathbb{R}^{(u-1)\times(u-1)}$ y $\widetilde{A}_{u - 1} \in \mathbb{R}^{(n-u+1)\times(n-u+1)}$ son trivialmente diagonal dominantes.

                Podemos ver que realizar el paso $u$-ésimo del algoritmo de Eliminación Gaussiana sobre $A$ equivale a realizar el primer paso de este algoritmo sobre $\widetilde{A}_{u - 1}$, sin modificar el resto de la matriz. Dado que $A^{(u - 1)}_{u,u} \neq 0$, podremos realizar esto sin pivoteo, y del Lema 1 se sigue que el resultado de este proceso será diagonal dominante, lo cual implica directamente que también lo será $A^{(u)}$.

                Por último, probemos que la fila $u + 1$ de la matriz $A^{(u)}$ es no nula.
                \begin{itemize}
                    \item Si $1 \leq u + 1 \leq n$ o $mn < u + 1 \leq (m+1)n$, tendremos que la fila $u + 1$ tendrá un $1$ en la diagonal y $0$ en las demás posiciones, por lo que no se verá afectada por el algoritmo de Eliminación Gaussiana. Luego $A^{(u)}_{u+1,u+1} = 1$.
                    \item En caso contrario, por la forma en la que se construye la matriz del sistema, $A_{u+1,u+1+n}$ corresponde al coeficiente de la variable $t_{j,k+1}$ de la $u + 1$-ésima ecuación del sistema, que es trivialmente no nulo. Además, como notamos anteriormente, la matriz $A$ es \emph{banda} $n, n$. Por lo tanto, $A_{p,u+1+n} = 0$ para todo $p = 0, \dots, u$ y, por consiguiente, $A^{(u)}_{u+1,u+1+n} = A_{u+1,u+1+n}$. Esto implica que la fila $u + 1$ de la matriz $A^{(u)}$ es no nula, como queríamos probar. \qedhere
                \end{itemize}
            \end{itemize}

    \subsection{Interpretación de los resultados}

        \subsubsection{Estimación de la posición de la isoterma crítica}

            Una vez obtenida la resolución del sistema, fue necesario decidir un criterio para estimar la ubicación de la isoterma pedida. Algunas de las alternativas que consideramos al respecto fueron las siguientes:

            \begin{itemize}
                \item Para cada ángulo $\theta_k$, considerar como posición de la isoterma el radio correspondiente al punto $(r_j, \theta_k)$ de la discretización con el valor de $t_{j,k}$ más próximo a 500{\degree}C.
                \item Para cada ángulo $\theta_k$, considerar como posición de la isoterma el radio del punto de la discretización inmediatamente exterior al primero con temperatura mayor o igual a 500{\degree}C, contando desde la pared externa. Es decir, considerar el menor radio $r_j$ tal que $t_{j',k} < 500$ para todo $j' \geq j$. La ventaja de este método es que asegura que la isoterma nunca se encontrará más cerca de la pared externa que el resultado arrojado, garantizando que se detectarán todas las posibles situaciones de riesgo.
                \item Para cada ángulo $\theta_k$, considerar $r_j$, el radio del punto más externo de la discretización con temperatura mayor o igual a 500{\degree}C. Luego, sabiendo que la isoterma buscada se encontrará entre este radio y el inmediatamente exterior, aproximar la variación de la temperatura en los puntos intermedios mediante una función lineal y utilizar esta aproximación para estimar la posición de la isoterma. Más precisamente, si $r_{iso}$ es el radio de la isoterma buscada, entonces
                    \[ r_{iso} = r_j + \Delta r \left(\frac{500 - t_{j,k}}{t_{j+1,k} - t_{j,k}} \right) \]
            \end{itemize}

            Finalmente, decidimos aplicar este último criterio, porque consideramos que brinda información más precisa que los otros dos, al tener en cuenta el valor la diferencia de temperatura entre los puntos de la discretización más cercanos a la isoterma buscada y la isoterma en si misma, información que los dos primeros métodos descartan. De esta manera, nos permite detectar situaciones de riesgo que con el primer método pasarían desapercibidas, pero evitando la generación de falsos positivos que provocaría el segundo método, que es mucho más estricto.

        \subsubsection{Cálculo del índice de peligrosidad}

            Para elegir la medida de peligrosidad lo que hacemos es tomar de todos los valores por los que pasa la isoterma el mas próximo a la pared externa del alto horno. A ese valor lo dividimos por el radio externo, dando así la relación entre la distancia mas cercana a la pared externa de la isoterma y la distancia total.
            Este es un numero entre 1 y 0 donde 0 es el centro del horno y 1 es la pared externa. Cuanto mas cercano a 1 sea el valor, mas peligro de colapsar tiene. 

    \subsection{Implementación}

        Los dos métodos utilizados fueron implementados en lenguaje \texttt{C++}. (...)